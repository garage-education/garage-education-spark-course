\begin{frame}
    \frametitle{Spark Distributed Execution}
    \begin{itemize}
        \item Spark Application.
        \item Spark driver.
        \item Spark session.
        \item Cluster manager.
        \item Spark executors.
        \item Deployment mode.
        \item Data partition.
    \end{itemize}
\end{frame}

% Ch.04-13   | Spark Applications

\subsection{Spark Application}\label{subsec:spark-application}
\begin{frame}
    \frametitle{Spark Distributed Execution: Spark Application}

    \begin{itemize}
        \item A Spark application is a program designed for distributed data processing using Spark.
        \item It includes a driver program to run the main function and execute parallel operations on a cluster.
        \item The application divides processing tasks into smaller units called tasks, distributed across cluster nodes.
        \item Spark applications support various languages like Scala, Java, Python, and R.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Spark Application}

    \includegraphics[width=\textwidth,height=.75\textheight,keepaspectratio]{./Figures/chapter-04/SparkCourse.drawio}\footnote{Spark the definitive guide, Ch.02}

\end{frame}

% Ch.04-14   | Spark Driver

\subsection{Spark Driver}\label{subsec:spark-driver}

\begin{frame}
    \frametitle{Spark Driver}
    The driver is the process \texttt{\color{blue}in the driver seat.}\footnote{Spark: The Definitive Guide, Chapter 15.} of your Spark Application.

\end{frame}

\begin{frame}
    \frametitle{Spark Driver: Key functions}


    \begin{itemize}
        \item It transforms all the Spark operations into DAG computations, schedules them, and distributes their execution as tasks across the Spark executors.
        \item Controlling the execution of a Spark Application.

    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Spark Driver: Key functions}

    \begin{itemize}
        \item Acting as a process on a physical machine, responsible for the overall state of the application on the cluster.
        \item It instantiates the \texttt{\color{blue} SparkSession}
        \item It requests resources  (CPU, memory, etc.) from the cluster manager for Sparkâ€™s executors (JVMs).
        \item Once the resources are allocated, it communicates directly with the executors.
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Spark Driver: Recap}
%graph TD;
%    A[Spark Driver] -->|Schedules Tasks| B(DAG Scheduler);
%    A -->|Request Resources| C(Cluster Manager);
%    A -->|Communicates| D(Executors);
%    A -->|Initiates| E(SparkSession);
%    B -->|Distributes Tasks| D;
%    C -->|Grants Resources| D;
%    D -->|Executes Tasks| F(Task);
%    E -->|User Interface| G(Application Code);
%
    \includegraphics[width=\textwidth,height=.85\textheight,keepaspectratio]{./Figures/chapter-04/Mairmaid_SparkDriver}

\end{frame}

% Ch.04-15   | Spark Session

\subsection{Spark Distributed Execution: SparkSession}\label{subsec:spark-session}
\begin{frame}
    \frametitle{What is a Session?}

    \begin{itemize}
        \item A session refers to an interaction between two or more entities.
        \item In computing, it's especially common in networked computers on the internet.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Types of Sessions in Computing}

    \begin{itemize}
        \item TCP session: A basic form of interaction in network communication.
        \item Login session: The period when a user is logged into a system.
        \item HTTP session: A series of interactions between a web server and a client.
        \item User session: The time a user interacts with a software application.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Introducing SparkSession}

    \begin{itemize}
        \item Similar to the sessions mentioned, Spark has its own SparkSession.
        \item SparkSession provides a unified entry point to Spark's functionalities.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Functionality of SparkSession}

    \begin{itemize}
        \item \textbf{SparkSession:} An object that provides a point of entry to interact with underlying Spark functionality.
        \item It allows programming Spark with its APIs.
        \item In an interactive Spark shell, the Spark driver instantiates a SparkSession for you.
        \item In a Spark application, you create a SparkSession object yourself.
        \item You can program Spark using DataFrame and Dataset APIs through SparkSession.
        \item In Scala and Python, the variable is available as \texttt{spark} when you start the console.

    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SparkSession}
    \begin{itemize}
        \item The SparkSession instance is the way Spark executes user-defined manipulations across the cluster.
        \item There is a one-to-one correspondence between a SparkSession and a Spark Application.
        \item It connects the Spark driver program with the cluster manager.
        \item SparkSession determines the resource manager (YARN, Mesos, or Standalone) for communication.
        \item It allows configuration of Spark parameters.
    \end{itemize}
\end{frame}

%
%\begin{frame}[fragile]
%    \frametitle{Interacting with Spark in Earlier Versions}
%
%    \begin{itemize}
%        \item In earlier versions of Spark, setting up a Spark application required creating a SparkConf and SparkContext.
%    \end{itemize}
%
%    \begin{lstlisting}[language=Python,label={lst:pyspark-spark-context},caption={Create SparkContext in old Spark versions}]
%from pyspark import SparkConf, SparkContext
%from pyspark.sql import SQLContext
%
%sparkConf = SparkConf().setAppName("SparkSessionExample").setMaster("local")
%sc = SparkContext(conf=sparkConf)
%sqlContext = SQLContext(sc)
%    \end{lstlisting}
%
%\end{frame}
%
%
%\begin{frame}[fragile]
%    \frametitle{Interacting with Spark in Earlier Versions}
%
%    \begin{lstlisting}[language=Scala,label={lst:scala-spark-context},caption={Scala: Create SparkContext in old Spark versions}]
%//set up the spark configuration and create contexts
%val sparkConf = new SparkConf().setAppName("SparkSessionZipsExample").setMaster("local")
%val sc = new SparkContext(sparkConf)
%sc.set("spark.some.config.option", "some-value")
%val sqlContext = new org.apache.spark.sql.SQLContext(sc)
%    \end{lstlisting}
%
%\end{frame}
%
%
%
%\begin{frame}[fragile]
%    \frametitle{Simplification in Spark 2.0 with SparkSession}
%
%    \begin{itemize}
%        \item Spark 2.0 introduced SparkSession, simplifying the way you interact with Spark.
%        \item SparkSession encapsulates SparkConf, SparkContext, and SQLContext.
%    \end{itemize}
%
%    \begin{lstlisting}[language=Python,label={lst:spark-session-python-example},caption={Pyspark: Create SparkSession}]
%from pyspark.sql import SparkSession
%
%spark = SparkSession.builder \
%    .appName("SparkSessionExample") \
%    .config("spark.some.config.option", "value") \
%    .getOrCreate()
%    \end{lstlisting}
%\end{frame}
%
%\begin{frame}[fragile]
%    \frametitle{Simplification in Spark 2.0 with SparkSession}
%    \begin{lstlisting}[language=Scala,label={lst:spark-session-scala-example},caption={Spark: Create SparkSession}]
%// Create a SparkSession. No need to create SparkContext.
%val warehouseLocation = "file:${system:user.dir}/spark-warehouse"
%val spark = SparkSession
%   .builder()
%   .appName("SparkSessionZipsExample")
%   .config("spark.sql.warehouse.dir", warehouseLocation)
%   .enableHiveSupport()
%   .getOrCreate()
%    \end{lstlisting}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Using SparkSession}
%    \begin{itemize}
%        \item Spark 2.0 introduces SparkSession.
%        \item With SparkSession, you can access all Spark functionalities.
%        \item A unified entry point to Spark's functionality, reduces the need for multiple context initializations.
%        \item Encapsulates the functionalities of SQLContext, HiveContext, and more.
%    \end{itemize}
%\end{frame}
%%
%%\begin{frame}
%%    \frametitle{Reference}
%%    \begin{itemize}
%%        \item The examples shown are based on a blog post from Databricks.
%%        \item URL: \url{https://www.databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html}
%%    \end{itemize}
%%\end{frame}
%%
%%%Ch.04-16   | Spark Cluster Manager
%%
%%\subsection{Cluster manager}\label{subsec:cluster-manager}
%%
%%\begin{frame}
%%    \frametitle{Introduction to Spark Cluster Managers}
%%    \begin{itemize}
%%        \item The cluster manager allocates resources for Spark applications.
%%        \item Supports several managers: Standalone, Hadoop YARN, Apache Mesos, and Kubernetes.
%%    \end{itemize}
%%\end{frame}
%%
%%\begin{frame}
%%    \frametitle{Role of the Cluster Manager}
%%    \begin{itemize}
%%        \item The Spark Driver and Executors do not exist in a void, and this is where the cluster manager
%%        comes in.
%%        \item The cluster manager is important for managing a cluster of machines intended to run Spark Applications.
%%        \item Maintains a \texttt{driver (or master)} and \texttt{worker} nodes, tied to \textbf{physical machines}.
%%    \end{itemize}
%%\end{frame}
%%
%%
%%\begin{frame}
%%    \frametitle{Cluster Manager Components}
%%    \begin{figure}
%%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/cluster_manager_processes}
%%        \caption{A cluster driver and worker (no Spark Application yet).}\label{fig:cluster_manager_processes}
%%    \end{figure}
%%\end{frame}
%%
%%\begin{frame}
%%    \frametitle{Execution of Spark Applications}
%%    \begin{itemize}
%%        \item The user requests resources from the cluster manager to initiate Spark applications.
%%        \item The user configures the application to specify resources for the driver or only for executors.
%%        \item The cluster manager directly manages the machines during the execution of the application.
%%    \end{itemize}
%%\end{frame}
%%
%%% Ch.04-17   | Spark Execution Mode
%%
%%\subsection{Execution Modes}\label{subsec:deployment-mode}
%%\begin{frame}
%%    \frametitle{Execution Modes Overview}
%%    \begin{itemize}
%%        \item Execution modes define the location of resources when running Spark applications.
%%        \item Three modes available:
%%        \begin{enumerate}
%%            \item Cluster mode
%%            \item Client mode
%%            \item Local mode
%%        \end{enumerate}
%%    \end{itemize}
%%\end{frame}
%%
%%\begin{frame}
%%    \frametitle{Cluster Manager Components}
%%    \begin{figure}
%%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/cluster_manager_processes}
%%        \caption{A cluster driver and worker (no Spark Application yet).}\label{fig:cluster-manager-components}
%%    \end{figure}
%%\end{frame}
%%
%%\begin{frame}
%%    \frametitle{Cluster Mode}
%%    \begin{itemize}
%%        \item Most common mode for running Spark Applications.
%%        \item User submits a pre-compiled JAR, Python script, or R script to a cluster manager.
%%        \item The cluster manager then launches the driver process on a worker node inside the cluster.
%%        \item Executor processes also launched within the cluster.
%%        \item Cluster manager handles all Spark Application processes.
%%        \item This means that the cluster manager is responsible for maintaining all Spark Applicationâ€“related processes.
%%    \end{itemize}
%%\end{frame}
%%
%\begin{frame}
%    \frametitle{Spark Cluster Mode}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/spark_cluster_mode}
%        \caption{Sparkâ€™s cluster mode.}\label{fig:cluster_mode}
%    \end{figure}
%\end{frame}
%%
%\begin{frame}
%    \frametitle{Client Mode}
%    \begin{itemize}
%        \item Similar to cluster mode, but the \textbf{\texttt{Spark driver remains on the client
%        machine that submitted the application}}.
%        \item \textbf{Client machine} is responsible for maintaining the Spark driver process.
%        \item \textbf{Cluster manager} maintains executor processes.
%        \item Commonly used with gateway machines or edge nodes.
%        \item The driver is running on a machine outside of the cluster but that the workers are located on machines in the cluster.
%    \end{itemize}
%\end{frame}
%%
%\begin{frame}
%    \frametitle{Spark Client Mode}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/spark_client_mode}
%        \caption{Sparkâ€™s client mode.}\label{fig:client_mode}
%    \end{figure}
%\end{frame}
%%
%\begin{frame}
%    \frametitle{Local Mode}
%    \begin{itemize}
%        \item Runs the entire application on a single machine.
%        \item Parallelism achieved through threads on the same machine.
%        \item Ideal for learning, testing, or local development.
%        \item Not recommended for production use.
%    \end{itemize}
%\end{frame}
%
%%\begin{frame}
%%    \frametitle{Spark Deployment Modes Cheat Sheet}
%%    \begin{table}[h!]
%%        \centering
%%        \resizebox{\textwidth}{!}{%
%%        \begin{tabular}{|p{2cm} |p{5cm} |p{5cm} |p{5cm}|}
%%            \hline
%%            \rowcolor{Gray}
%%            \textbf{Mode} & \textbf{Driver} & \textbf{Executor} & \textbf{Cluster Manager} \\
%%            \hline
%%            \textbf{Local} & Runs on a single JVM & Runs with driver & Single host \\
%%            \hline
%%            \textbf{Standalone} & Any cluster node & Own JVM per node & Any host in cluster \\
%%            \hline
%%            \textbf{YARN (client)} & Client machine & NodeManager container & YARN Resource Manager \\
%%            \hline
%%            \textbf{YARN (cluster)} & YARN App Master & Similar to client & Similar to client \\
%%            \hline
%%            \textbf{Kubernetes} & Kubernetes pod & Individual pods & Kubernetes Master \\
%%            \hline
%%        \end{tabular}
%%        }
%%        \caption{Summary of Spark deployment modes}\label{tab:spark-deployment}
%%    \end{table}
%%\end{frame}
%% Ch.04-18   | Spark Executors
%
%\subsection{Spark executors}\label{subsec:spark-executors}
%
%\begin{frame}
%    \frametitle{Spark Executors in Depth}
%    \begin{itemize}
%        \item Executors are processes that run the tasks assigned by the driver.
%        \item Each Spark Application has distinct executor processes.
%        \item Typically, one executor runs per node in most deployment modes.
%        \item Executors' main function: Execute tasks, return status, and communicate outcomes.
%    \end{itemize}
%\end{frame}
%
%% Ch.04-19   | Spark Data Partitioning
%
%\subsection{Data partition}\label{subsec:data-partition}
%
%
%% Slide 1
%\begin{frame}{Introduction to Data Distribution and Partitions}
%    \begin{itemize}
%        \item \textbf{Data Distribution:} Physical data is distributed across storage as partitions residing in either HDFS or cloud storage.
%        \item \textbf{Data Abstraction:} Spark treats each partition as a high-level logical data abstractionâ€”as a DataFrame in memory.
%    \end{itemize}
%\end{frame}
%
%% Slide 2
%\begin{frame}{Data Locality and Task Allocation}
%    \begin{itemize}
%        \item \textbf{Data Locality:} Each Spark executor is preferably allocated a task that requires it to read the partition closest to it in the network, observing data locality.
%        \item \textbf{Optimal Task Allocation:} Partitioning allows for efficient parallelism.
%        \item \textbf{Minimize Network Bandwidth:} A distributed scheme of breaking up data into chunks or partitions allows Spark executors to process only data that is close to them, minimizing network bandwidth.
%    \end{itemize}
%\end{frame}
%
%% Slide 3
%\begin{frame}{Benefits of Partitioning}
%    \begin{itemize}
%        \item \textbf{Efficient Parallelism:} Partitioning allows executors to process data close to them.
%        \item \textbf{Dedicated Processing:} Each core on an executor works on its own partition, minimizing network bandwidth usage.
%    \end{itemize}
%\end{frame}
%
%% Slide 4
%\begin{frame}[fragile]{Practical Example - Distributing Data}
%    \begin{lstlisting}[language=Python,label={lst:pyspark-data-partitioning}]
%        log_df = spark.read.text("path_to_large_text_file").repartition(8)
%        print(log_df.rdd.getNumPartitions())
%    \end{lstlisting}
%    This example splits data across clusters into eight partitions.
%\end{frame}
%
%% Slide 5
%\begin{frame}[fragile]{Practical Example - Creating a DataFrame}
%    \begin{lstlisting}[language=Python,label={lst:pyspark-creating-dataframe}]
%        df = spark.range(0, 10000, 1, 8)
%        print(df.rdd.getNumPartitions())
%    \end{lstlisting}
%    This creates a DataFrame of 10,000 integers over eight partitions in memory.
%\end{frame}
%
%% Slide 6
%\begin{frame}{Conclusion}
%    \begin{itemize}
%        \item \textbf{Key Takeaway:} Efficient data partitioning is crucial for optimizing processing in Spark.
%    \end{itemize}
%\end{frame}
%
%% Ch.04-20   | Spark Operations
%
%\subsection{Spark Operations}\label{subsec:spark-operations}
%\begin{frame}
%    \frametitle{Spark Operations}
%    \begin{itemize}
%        \item Spark \textbf{operations} on distributed data can be classified into two types: transformations
%        and actions.
%        \item All spark operations are \texttt{\color{blue}{immutable}}.
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle{Immutable Objects}
%    \begin{itemize}
%        \item An object whose state cannot change after it has been constructed is
%        called immutable (unchangable).\footnote{Referenced from https://otfried.org/courses/cs109scala/tutorial_mutable.html}
%        \item  The methods of an immutable object do not modify the state of the object.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Immutable Objects}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/Immutable_df}
%        \caption{Spark Dataframe is immutable, and you can't change its values.}\label{fig:Immutable_df}
%    \end{figure}
%\end{frame}
%
%\begin{frame}[fragile]
%    \frametitle{Immutable Objects}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.75\textheight,keepaspectratio]{./Figures/chapter-04/pyspark_immutable_df}
%        \caption{Filtering a PySpark DataFrame Based on Age}\label{fig:pyspark_immutable_df}
%    \end{figure}
%\end{frame}
%\begin{frame}[fragile]
%    \frametitle{DEMO}
%    DEMO
%\end{frame}
%\begin{frame}
%    \frametitle{Spark Operations: Transformations}
%    \begin{itemize}
%        \item Transformations: transform a Spark DataFrame into a new DataFrame \textit{\color{blue}{without altering the original data}}.
%        \item Example of Spark transformations, \texttt{\color{orange}{map(), select(), filter(), or drop()} }
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Transformations: What are Lazy Transformations?}
%    \begin{itemize}
%        \item In Spark, transformations are \textit{lazy}.
%        \item This means computations are not executed immediately.
%        \item Spark builds a \textbf{DAG} (Directed Acyclic Graph) of transformations.
%        \item All Transformations results are not computed immediately, but they are recorded or remembered as a \textbf{lineage}.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Transformations: Benefits of Lazy Evaluation}
%    \begin{itemize}
%        \item \textbf{Optimization:} A lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations, coalesce
%        them, or optimize transformations into stages for more efficient execution.
%        \item \textbf{Resource Management:} Executes tasks efficiently, using fewer resources.
%        \item \textbf{Fault Tolerance:} Easier to recompute parts of the pipeline if a part fails.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Transformations: Lazy Transformation}
%    \begin{itemize}
%        \item Consider a dataset with map and filter transformations.
%        \item Spark does not execute these transformations when they are defined.
%        \item Transformations are executed when an action (like \textbf{collect}, \textbf{count}) is called.
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle{Lazy Transformations Example}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.75\textheight,keepaspectratio]{./Figures/chapter-04/pyspark_transformations}
%        \caption{Spark Lazy Transformations Example.}\label{fig:pyspark_transformations}
%    \end{figure}
%\end{frame}
%\begin{frame}
%    \frametitle{Spark Operations: Actions}
%    \begin{itemize}
%        \item An action triggers the lazy evaluation of all the recorded transformations.
%        \item Actions are operations that trigger execution of transformations.
%        \item They are used to either compute a result to be returned to the Spark driver program or to write data to an external storage system.
%        \item Actions include operations like \textbf{count}, \textbf{collect}, \textbf{saveAsTextFile}, and \textbf{take}.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Examples of Spark Actions}
%    \begin{itemize}
%        \item \textbf{collect()}: Collects all elements from the Spark context to the driver program.
%        \item \textbf{count()}: Returns the number of elements in the dataset.
%        \item \textbf{saveAsTextFile(path)}: Saves the dataset to a text file at the specified path.
%        \item \textbf{take(n)}: Returns an array with the first n elements of the dataset.
%    \end{itemize}
%\end{frame}
%
%% Ch.04-21   | Transformations Narrow Vs Wide
%
%\subsection{Narrow and Wide Transformations}\label{subsec:narrow-and-wide-transformations}
%\begin{frame}
%    \frametitle{Introduction to Spark Transformations}
%    \begin{itemize}
%        \item Transformations create new RDDs from existing ones.
%        \item Spark has two types of transformations: Narrow and Wide.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{What are Narrow Transformations?}
%    \begin{itemize}
%        \item Transformations that do not require data shuffling between partitions.
%        \item Examples: \texttt{map()}, \texttt{filter()}.
%        \item Data processing is limited to a single partition.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{What are Narrow Transformations?}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.75\textheight,keepaspectratio]{./Figures/chapter-04/Narrow}
%        \caption{Spark Narrow Transformations.}\label{fig:spark_narrow}
%    \end{figure}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Benefits of Narrow Transformations}
%    \begin{itemize}
%        \item Efficient with minimal data movement.
%        \item Best for independent data processing tasks.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{What are Wide Transformations?}
%    \begin{itemize}
%        \item Transformations that involve shuffling data across partitions.
%        \item Examples: \texttt{groupBy()}, \texttt{reduceByKey()}.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{What are Wide Transformations?}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.75\textheight,keepaspectratio]{./Figures/chapter-04/Wide}
%        \caption{Spark Wide Transformations.}\label{fig:spark_wide}
%    \end{figure}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Wide Transformations and Dependencies}
%    \begin{itemize}
%        \item \textbf{Wide Dependencies}: Require data from multiple partitions, often involving shuffling.
%        \item \textbf{Examples}: \texttt{groupBy()}, \texttt{orderBy()} - data is combined across partitions, affecting performance.
%        \item \textbf{Impact}: These transformations are necessary for operations like counting occurrences across a dataset.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Implications of Wide Transformations}
%    \begin{itemize}
%        \item Shuffling can be expensive in terms of time and network I/O.
%        \item Essential for aggregation and grouping operations.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Narrow vs. Wide Dependencies}
%    \begin{itemize}
%        \item \textbf{Narrow Dependencies}: A single output partition can be computed from a single input partition without data exchange.
%        \item \textbf{Examples}: \texttt{filter()}, \texttt{contains()} - operate independently on partitions.
%    \end{itemize}
%\end{frame}
%
%\subsection{Repartition vs. Coalesce}\label{subsec:repartition-vs-coalesce}
%\begin{frame}
%    \frametitle{Repartition vs. Coalesce}
%    \begin{itemize}
%        \item In Apache Spark, repartition and coalesce are two methods used to change the number of partitions in an RDD (Resilient
%        Distributed Dataset).
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Repartition vs. Coalesce}
%
%    \begin{table}[h!]
%        \centering
%        \resizebox{\textwidth}{!}{%
%            \begin{tabular}{|p{2cm} |p{6cm} |p{6cm} |}
%                \hline
%                \rowcolor{Gray}
%                \hline
%                \textbf{Aspect}    & \textbf{Repartition}                                                                                                & \textbf{Coalesce}                                                                                                         \\
%                \hline
%                \textbf{Purpose}   & \textcolor{blue}{Increases or decreases} the number of partitions. & \textcolor{blue}{Decreases} the number of partitions. \\
%                \hline
%                \textbf{Mechanism} & \textcolor{blue}{Shuffles all} the data across the network to create a new set of partitions. & \textcolor{blue}{Merges existing} partitions \textcolor{blue}{without} a full data shuffle. \\
%                \hline
%                \textbf{Use Case}  & Ideal for increasing the number of partitions or significantly \textcolor{blue}{changing the distribution} of data. & Efficient for \textcolor{blue}{reducing the number} of partitions when the target number is less than the current number. \\
%                \hline
%                \textbf{Cost}      & Expensive due to the \textcolor{blue}{full data shuffle}.                                                         & Less expensive than \texttt{repartition} as it \textcolor{blue}{minimizes data movement}.                               \\
%                \hline
%            \end{tabular}
%        }
%        \caption{Comparison of Repartition and Coalesce in Apache Spark}\label{tab:rerepartition-coalesce}
%    \end{table}
%\end{frame}
%
%\begin{frame}[fragile]
%    \frametitle{High-Level Code: Repartition}
%    \begin{lstlisting}[language=scala,label={lst:rep-partition},caption={Repartition Code}]
%def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
%  coalesce(numPartitions, shuffle = true)
%}
%    \end{lstlisting}
%    \begin{itemize}
%        \item \textbf{Key Points}:
%        \begin{itemize}
%            \item \textbf{Shuffle}: Always performs a shuffle by calling `coalesce` with `shuffle = true`.
%            \item \textbf{Usage}: Suitable for both increasing and decreasing the number of partitions with even data distribution.
%        \end{itemize}
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{High-Level Code: Repartition}
%
%        \begin{itemize}
%            \item \textbf{Operation}: Calls `coalesce` with `shuffle = true`.
%            \item \textbf{Process}: Full data shuffle across the network to redistribute data evenly.
%            \item \textbf{Impact on Logs}:
%            \begin{itemize}
%                \item Increased log entries due to extensive shuffle operations.
%                \item Higher overhead and cost.
%                \item Useful for balancing load and increasing parallelism.
%            \end{itemize}
%        \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{High-Level Code: Repartition}
%    \begin{itemize}
%        \item Shuffling Logic (If shuffle is true):
%
%            \begin{itemize}
%                \item Data is shuffled to distribute it evenly across the new partitions.
%                \item Each item is assigned a new partition based on a random function.
%                \item A new RDD is created with the shuffled data and the specified number of partitions.
%            \end{itemize}
%        \end{itemize}
%\end{frame}
%
%\begin{frame}[fragile]
%    \frametitle{High-Level Code: Coalesce}
%
%    \begin{lstlisting}[language=scala,basicstyle=\tiny,label={lst:coalesce},caption={Coalesce Code}]
%def coalesce(numPartitions: Int, shuffle: Boolean = false,
%             partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
%            (implicit ord: Ordering[T] = null): RDD[T] = withScope {
%  require(numPartitions > 0, s"Number of partitions ($numPartitions) must be positive.")
%  if (shuffle) {
%    // Shuffle logic
%    val distributePartition = (index: Int, items: Iterator[T]) => {
%      var position = new XORShiftRandom(index).nextInt(numPartitions)
%      items.map { t =>
%        position = position + 1
%        (position, t)
%      }
%    } : Iterator[(Int, T)]
%    new CoalescedRDD(new ShuffledRDD[Int, T, T](
%        mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true),
%        new HashPartitioner(numPartitions)),
%      numPartitions,
%      partitionCoalescer).values
%  } else {
%    // No-shuffle logic
%    new CoalescedRDD(this, numPartitions, partitionCoalescer)
%  }
%}
%    \end{lstlisting}
%
%\end{frame}
%
%\begin{frame}[fragile]
%    \frametitle{High-Level Code: Coalesce}
%    \begin{itemize}
%        \item \textbf{Key Points}:
%        \begin{itemize}
%            \item \textbf{Shuffle}: Optional; can perform a shuffle if `shuffle = true`.
%            \item \textbf{No Shuffle}: Default behavior (without shuffle) merges partitions locally.
%            \item \textbf{Usage}: Efficient for reducing the number of partitions without shuffling.
%        \end{itemize}
%    \end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}
%    \frametitle{High-Level Code: Coalesce}
%
%    \begin{itemize}
%        \begin{itemize}
%            \item \textbf{Operation}: Can operate with or without shuffle.
%            \item \textbf{Process}:
%            \begin{itemize}
%                \item \textbf{No Shuffle (default)}: Merges partitions locally.
%                \item \textbf{With Shuffle}: Performs a shuffle for even data distribution if specified.
%            \end{itemize}
%            \item \textbf{Impact on Logs}:
%            \begin{itemize}
%                \item Decreased log entries when merging locally without shuffle.
%                \item Lower overhead and cost.
%                \item Efficient for reducing partitions, especially useful for write operations.
%                \item Optional shuffle provides flexibility for balanced distribution.
%            \end{itemize}
%        \end{itemize}
%    \end{itemize}
%\end{frame}
%
%\subsection{Understanding Spark Application Concepts}\label{subsec:understanding-spark-application-concepts}
%
%\begin{frame}
%    \frametitle{Understanding Spark Application Concepts}
%    \begin{itemize}
%        \item \textbf{Job}: A parallel computation consisting of multiple tasks that gets spawned in response
%        to a Spark action (e.g., save(), collect()).
%        \item \textbf{Stage}: Each job gets divided into smaller sets of tasks called stages that depend on each
%        other.
%        \item \textbf{Task}: A single unit of work or execution that will be sent to a Spark executor.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Jobs}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/spark_job}
%        \caption{Spark driver creating one or more Spark jobs.}\label{fig:spark_job}
%    \end{figure}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Stages}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.65\textheight,keepaspectratio]{./Figures/chapter-04/Spark_Stages}
%        \caption{Spark job creating one or more stages}\label{fig:spark_stages}
%    \end{figure}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Spark Stages}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/Spark_Stage}
%        \caption{Spark stage creating one or more tasks to be distributed to executors}\label{fig:spark_stage}
%    \end{figure}
%\end{frame}

%\begin{frame}
%    \frametitle{Spark Tasks}
%    \begin{figure}
%        \includegraphics[width=\textwidth,height=.7\textheight,keepaspectratio]{./Figures/chapter-04/Spark_tasks}
%        \caption{Spark stage creating one or more tasks to be distributed to executors}\label{fig:Spark_tasks}
%    \end{figure}
%\end{frame}
%
%\subsection{Running Word Count \& Aggregation using Spark}\label{subsec:running-word-count-&-aggregation-using-spark}
%
%\subsection{Spark Application Life Cycle}\label{subsec:spark-application-life-cycle}
%%%%% Later
%\begin{frame}
%    \frametitle{The Life Cycle of a Spark Application (Inside Spark)}
%    \begin{itemize}
%        \item The Life Cycle of a Spark Application (Inside Spark)
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle{The Life Cycle of a Spark Application (Outside Spark)}
%    \begin{itemize}
%        \item The Life Cycle of a Spark Application (Outside Spark)
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle{The Life Cycle of a Spark Application (Inside Spark)}
%    \begin{itemize}
%        \item The Life Cycle of a Spark Application (Inside Spark)
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{ADVANCED: SPARK DRIVER INTERNAL SCHEDULER}
%    \begin{itemize}
%        \item GOING DEEPER INTO SPARK DRIVER.
%        \item CAN WE OPTIMIZE SPARK DRIVER WORKLOAD?
%    \end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\subsection{Further Readings and Assignment}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
% !TeX root = ../main.tex
%%% TeX-engine: xetex
%%% End: