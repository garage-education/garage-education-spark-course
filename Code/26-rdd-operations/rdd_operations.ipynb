{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccf76f1-e7b6-486d-bba3-9ed9748d8d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a88255-f6c0-4972-b395-6bccb46e4fed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bea342-ed95-4387-9cd0-d41be8b0f493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Map` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b664677a-d399-4a30-9e3a-379ec91939fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. map ###\nDescription: Return a new RDD by applying a function to all elements of this RDD.\n01 map example (multiply by 2): [2, 4, 6, 8, 10]\nexample_map example (word count in sentences): [2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "# 1. map\n",
    "print(\"### 1. map ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD.\")\n",
    "\n",
    "# Example 1: Multiply each element by 2\n",
    "simple_map = rdd.map(lambda x: x * 2).collect()\n",
    "print(\"01 map example (multiply by 2):\", simple_map)\n",
    "\n",
    "# Example 2: Extract the length of each word in a list of sentences\n",
    "sentences = [\"Hello world\", \"Apache Spark\", \"RDD transformations Wide Vs Narrow Spark\"]\n",
    "# Hello World => split (\" \") => [(0)-> Hello, (1) -> World]\n",
    "sentence_rdd = sc.parallelize(sentences)\n",
    "words_map = sentence_rdd.map(lambda sentence: len(sentence.split(\" \"))).collect()\n",
    "print(\"example_map example (word count in sentences):\", words_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1c213a-1574-43e1-8251-8836420c07d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Filter` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00eaeb0a-b58d-43f9-947f-e2d47787cb60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 2. filter ###\nDescription: Return a new RDD containing only the elements that satisfy a predicate.\n01 filter example (even numbers): [2, 4]\nexample_ filter example (sentences with 'Spark'): ['Apache Spark', 'RDD transformations Wide Vs Narrow Spark']\n"
     ]
    }
   ],
   "source": [
    "# 2. filter\n",
    "print(\"\\n### 2. filter ###\")\n",
    "print(\"Description: Return a new RDD containing only the elements that satisfy a predicate.\")\n",
    "\n",
    "# 01 Example: Filter out even numbers\n",
    "simple_filter = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "print(\"01 filter example (even numbers):\", simple_filter)\n",
    "\n",
    "# example_Example: Filter sentences containing the word 'Spark'\n",
    "words_filter = sentence_rdd.filter(lambda sentence: \"Spark\" in sentence).collect()\n",
    "print(\"example_ filter example (sentences with 'Spark'):\", words_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438692c8-0dd5-4bc0-89d8-460cdec835a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `FlatMap` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae10aa1-037a-4a4f-8d7d-0201a8837e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 3. flatMap ###\nDescription: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\n01 sentences_mapped: [['Hello', 'world'], ['Apache', 'Spark'], ['RDD', 'transformations', 'Wide', 'Vs', 'Narrow', 'Spark']]\n02 flatMap example (split sentences into words): ['Hello', 'world', 'Apache', 'Spark', 'RDD', 'transformations', 'Wide', 'Vs', 'Narrow', 'Spark']\nflatten_list  flatMap example (flatten list of lists): [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# 3. flatMap\n",
    "print(\"\\n### 3. flatMap ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\")\n",
    "\n",
    "# 01 Example: Split sentences into words\n",
    "sentences_mapped = sentence_rdd.map(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(\"01 sentences_mapped:\", sentences_mapped)\n",
    "\n",
    "simple_flatMap = sentence_rdd.flatMap(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(\"02 flatMap example (split sentences into words):\", simple_flatMap)\n",
    "\n",
    "# example_Example: Flatten a list of lists\n",
    "nested_lists = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "nested_rdd = sc.parallelize(nested_lists)\n",
    "flatten_list = nested_rdd.flatMap(lambda x: x).collect()\n",
    "print(\"flatten_list  flatMap example (flatten list of lists):\", flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0c9fa6-6ccf-465f-a7f5-b59bbc379577",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Reduce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f23341-169c-45e0-84a6-d9a030e27f20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 4. reduce ###\nDescription: Reduces the elements of this RDD using the specified commutative and associative binary operator.\n01 reduce example (sum of elements): 15\nreduce example (longest word): hippopotamus\n"
     ]
    }
   ],
   "source": [
    "# 4. reduce\n",
    "print(\"\\n### 4. reduce ###\")\n",
    "print(\"Description: Reduces the elements of this RDD using the specified commutative and associative binary operator.\")\n",
    "\n",
    "# 01 Example: Sum of elements\n",
    "simple_reduce = rdd.reduce(lambda x, y: x + y)\n",
    "print(\"01 reduce example (sum of elements):\", simple_reduce)\n",
    "\n",
    "# example_Example: Find the longest word in a list of words\n",
    "words = [\"cat\", \"elephant\", \"rat\", \"hippopotamus\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "words_rdd_reduced = words_rdd.reduce(lambda x, y: x if len(x) > len(y) else y)\n",
    "print(\"reduce example (longest word):\", words_rdd_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710eaaca-e6f8-44e4-8dc9-1b52f1284db8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `groupByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3989c5d-38c5-4685-af57-8264dbeb83f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 5. groupByKey ###\nDescription: Group the values for each key in the RDD into a single sequence.\n01 groupByKey example (group numbers): [(1, ['a', 'ali']), (2, ['b']), (3, ['c']), (4, ['d']), (5, ['e'])]\nwords_grouped example (group words by starting letter): [('elephant', [5, 20]), ('dog', [3]), ('cat', [1]), ('car', [2]), ('deer', [4])]\n"
     ]
    }
   ],
   "source": [
    "# 5. groupByKey\n",
    "print(\"\\n### 5. groupByKey ###\")\n",
    "print(\"Description: Group the values for each key in the RDD into a single sequence.\")\n",
    "\n",
    "# 01 Example: Group numbers by even and odd\n",
    "pairs = [(1, 'a'),(1, 'ali'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "simple_groupByKey = pairs_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"01 groupByKey example (group numbers):\", simple_groupByKey)\n",
    "\n",
    "# example_Example: Group words by their starting letter\n",
    "words_pairs = [(\"cat\", 1), (\"car\", 2), (\"dog\", 3), (\"deer\", 4), (\"elephant\", 5),(\"elephant\", 20)]\n",
    "words_rdd = sc.parallelize(words_pairs)\n",
    "# mapValues(list) converts the grouped values (which are iterable) into lists.\n",
    "words_grouped = words_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"words_grouped example (group words by starting letter):\", words_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe0346e-04cd-4f97-9c9e-35bfcd876ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `reduceByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c3a0324-15ec-45c0-903e-a2f722cc354b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 6. reduceByKey ###\nDescription: Merge the values for each key using an associative and commutative reduce function.\n01 reduceByKey example (sum values by key): [(1, 'a_a'), (2, 'b_b'), (3, 'c'), (4, 'd'), (5, 'e')]\nexample_ reduceByKey example (word count): [('elephant', 1), ('dog', 3), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 6. reduceByKey\n",
    "print(\"\\n### 6. reduceByKey ###\")\n",
    "print(\"Description: Merge the values for each key using an associative and commutative reduce function.\")\n",
    "pairs = [(1, 'a'),(1, '_a'), (2, 'b'), (2, '_b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# 01 Example: Sum values with the same key\n",
    "simple_reduceByKey = pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"01 reduceByKey example (sum values by key):\", simple_reduceByKey)\n",
    "\n",
    "# example_Example: Count the occurrences of each word in a list\n",
    "word_list = [\"cat\", \"cat\", \"dog\", \"elephant\", \"dog\", \"dog\"]\n",
    "word_pairs_rdd = sc.parallelize(word_list).map(lambda word: (word, 1))\n",
    "example__reduceByKey = word_pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"example_ reduceByKey example (word count):\", example__reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c024f23f-0cf9-46b7-b6dd-ddde4fdfa637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `join` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6ea909-41f4-420e-b56d-497b23d17441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 7. join ###\nDescription: Perform an inner join of this RDD and another one.\n01 join fruits_color_join (join two RDDs): [(1, ('apple', 'red')), (2, ('banana', 'yellow'))]\njoin example (employee-department join): [(1, ('John', 'HR')), (2, ('Jane', 'Finance'))]\n"
     ]
    }
   ],
   "source": [
    "# 7. join\n",
    "print(\"\\n### 7. join ###\")\n",
    "print(\"Description: Perform an inner join of this RDD and another one.\")\n",
    "\n",
    "# 01 Example: Join two RDDs by key\n",
    "fruits = sc.parallelize([(1, \"apple\"), (2, \"banana\")])\n",
    "colors = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "fruits_color_join = fruits.join(colors).collect()\n",
    "print(\"01 join fruits_color_join (join two RDDs):\", fruits_color_join)\n",
    "\n",
    "# example_Example: Join employee data with department data\n",
    "employees = sc.parallelize([(1, \"John\"), (2, \"Jane\"), (3, \"Joe\")])\n",
    "departments = sc.parallelize([(1, \"HR\"), (2, \"Finance\")])\n",
    "employees_department_join = employees.join(departments).collect()\n",
    "print(\"join example (employee-department join):\", employees_department_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169c5a2e-a955-48f3-8e31-116ea098dc3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `cogroup` Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9add167-5a88-4fd2-a673-b84029411589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TableA:\n",
    "\n",
    "| id | value  |\n",
    "|----|--------|\n",
    "|  1 | apple  |\n",
    "|  2 | banana |\n",
    "|  3 | orange |\n",
    "\n",
    "\n",
    "TableB:\n",
    "\n",
    "| id | color  |\n",
    "|----|--------|\n",
    "|  1 | red    |\n",
    "|  2 | yellow |\n",
    "\n",
    "\n",
    "Result of cogroup:\n",
    "\n",
    "| id | value  | color  |\n",
    "|----|--------|--------|\n",
    "|  1 | apple  | red    |\n",
    "|  2 | banana | yellow |\n",
    "|  3 | orange | NULL   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7d171e-b75a-47b7-92a8-051050c1ba34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 8. cogroup ###\nDescription: Group data from two RDDs sharing the same key.\n01 cogroup example (group two RDDs): [(1, (['apple'], ['red'])), (2, (['banana'], ['yellow'])), (3, (['orange'], []))]\nexample_cogroup example (sales-targets cogroup): [('store2', ([200], [])), ('store3', ([], [250])), ('store1', ([100], [150]))]\n"
     ]
    }
   ],
   "source": [
    "# 8. cogroup\n",
    "# The cogroup function in PySpark is used to group data from two RDDs that share the same key. \n",
    "# It combines the values of matching keys from both RDDs into a tuple of lists.\n",
    "print(\"\\n### 8. cogroup ###\")\n",
    "print(\"Description: Group data from two RDDs sharing the same key.\")\n",
    "\n",
    "# 01 Example: Cogroup two RDDs\n",
    "fruits_rdd = sc.parallelize([(1, \"apple\"), (2, \"banana\"), (3, \"orange\")])\n",
    "colors_rdd = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "cogrouped_fruits_colors = fruits_rdd.cogroup(colors_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"01 cogroup example (group two RDDs):\", cogrouped_fruits_colors)\n",
    "\n",
    "\n",
    "\n",
    "# example_Example: Cogroup sales data with target data\n",
    "sales_rdd = sc.parallelize([(\"store1\", 100), (\"store2\", 200)])\n",
    "targets_rdd = sc.parallelize([(\"store1\", 150), (\"store3\", 250)])\n",
    "cogrouped_sales_targets = sales_rdd.cogroup(targets_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"example_cogroup example (sales-targets cogroup):\", cogrouped_sales_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4091e572-2689-4000-bdb9-4e4323626ae3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `distinct` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c5dc53-6ef2-4cbc-886e-085d9e2e8a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 9. distinct ###\nDescription: Return a new RDD containing the distinct elements in this RDD.\nexample_distinct example (unique words): ['elephant', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# 9. distinct\n",
    "print(\"\\n### 9. distinct ###\")\n",
    "print(\"Description: Return a new RDD containing the distinct elements in this RDD.\")\n",
    "\n",
    "# example_Example: Unique words from a list of words\n",
    "words = [\"cat\", \"dog\", \"cat\", \"elephant\", \"dog\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "example__distinct = words_rdd.distinct().collect()\n",
    "print(\"example_distinct example (unique words):\", example__distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447ba6d1-dbae-401b-b704-a82838d04839",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `repartition` Vs. `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3331126a-c32a-4dda-adf8-c69cc5c7fcbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------------------+\n|user_id |action  |item_id|timestamp          |\n+--------+--------+-------+-------------------+\n|gn6o7l44|logout  |KGLZK  |2024-05-16 21:02:40|\n|wtyzg6kd|purchase|2TM09  |2024-05-25 07:59:17|\n|gw874pba|logout  |7FKCH  |2024-05-23 15:05:04|\n|q58k6f2j|logout  |W1PV8  |2024-05-08 04:35:12|\n|hqdfhtzv|click   |0JX6B  |2024-05-17 15:43:08|\n|s39ffvi4|login   |34JRH  |2024-05-30 14:53:57|\n|r2mza25t|login   |CGOJ8  |2024-05-19 11:08:39|\n|krq030ed|click   |JED4X  |2024-06-03 03:36:20|\n|agk3bhc2|click   |6RPA2  |2024-05-18 05:56:00|\n|e9f4x48y|logout  |2VFBK  |2024-06-02 08:09:44|\n+--------+--------+-------+-------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random log entry\n",
    "def generate_log_entry():\n",
    "    user_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "    action = random.choice([\"login\", \"logout\", \"purchase\", \"click\", \"view\"])\n",
    "    item_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
    "    timestamp = (datetime.now() - timedelta(seconds=random.randint(0, 2592000))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return (user_id, action, item_id, timestamp)\n",
    "\n",
    "# Generate synthetic data\n",
    "log_entries = [generate_log_entry() for _ in range(1000000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"action\", \"item_id\", \"timestamp\"]\n",
    "log_df = spark.createDataFrame(log_entries, columns)\n",
    "\n",
    "# Show sample data\n",
    "log_df.show(10, truncate=False)\n",
    "\n",
    "# Save to a CSV file in the DBFS (Databricks File System)\n",
    "log_df.write.csv(\"/tmp/user_logs\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb878915-824f-470a-8e56-88777b230ad0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `repartition` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483a552a-e31a-4e2c-aef8-db1caf8cc0f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 10. repartition ###\nDescription: Return a new RDD that has exactly numPartitions partitions.\nInitial Partitions: 8\nNew Partitions after Repartition: 100\n"
     ]
    }
   ],
   "source": [
    "# 10. repartition\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L480\n",
    "print(\"\\n### 10. repartition ###\")\n",
    "print(\"Description: Return a new RDD that has exactly numPartitions partitions.\")\n",
    "\n",
    "logs_rdd = sc.textFile(\"/tmp/user_logs\")\n",
    "\n",
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# Repartition to 200 partitions\n",
    "repartitioned_rdd = logs_rdd.repartition(100)\n",
    "new_partitions = repartitioned_rdd.getNumPartitions()\n",
    "print(f\"New Partitions after Repartition: {new_partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1beda3ee-a371-4625-88de-1e10d7c7ffbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e7ccc9-07d5-4465-8041-a75337563a22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 11. coalesce ###\nDescription: Return a new RDD that is reduced into numPartitions partitions.\nInitial Partitions: 8\nnew_partitions_1 after Coalesce: 2\nnew_partitions_2 after Coalesce: 8\n"
     ]
    }
   ],
   "source": [
    "# 11. coalesce\n",
    "print(\"\\n### 11. coalesce ###\")\n",
    "print(\"Description: Return a new RDD that is reduced into numPartitions partitions.\")\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L506\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala\n",
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# Coalesce to 4 partitions\n",
    "coalesced_rdd_1 = logs_rdd.coalesce(2)\n",
    "new_partitions_1 = coalesced_rdd_1.getNumPartitions()\n",
    "print(f\"new_partitions_1 after Coalesce: {new_partitions_1}\")\n",
    "\n",
    "# Coalesce to 50 partitions\n",
    "coalesced_rdd_2 = logs_rdd.coalesce(10)\n",
    "new_partitions_2 = coalesced_rdd_2.getNumPartitions()\n",
    "print(f\"new_partitions_2 after Coalesce: {new_partitions_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96caa3ba-53b5-45b0-a1fd-a7c08db697a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `sample` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3127e16-a092-4811-a732-0d7bae4b3d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 12. sample ###\nDescription: Return a sampled subset of this RDD.\n01 sample example (50% sample): [2, 5]\nComplex sample example (30% sample with replacement): [1, 3, 6, 8, 9, 16, 16, 17, 20, 27, 32, 38, 39, 42, 44, 44, 48, 49, 57, 60, 61, 63, 69, 73, 78, 82, 88, 90, 94, 102, 106, 107, 112, 115, 118, 124, 128, 129, 131, 132, 134, 137, 144, 146, 153, 156, 161, 163, 163, 171, 176, 179, 181, 182, 188, 189, 195, 197, 199, 201, 202, 202, 209, 209, 218, 219, 224, 225, 240, 254, 258, 260, 261, 261, 261, 263, 267, 272, 276, 280, 281, 286, 292, 296, 297, 297, 297, 299, 303, 304, 305, 306, 306, 308, 309, 315, 318, 324, 325, 327, 327, 328, 335, 336, 337, 338, 346, 348, 355, 356, 359, 359, 363, 364, 370, 373, 374, 384, 389, 389, 396, 398, 400, 400, 404, 411, 412, 413, 414, 419, 419, 422, 423, 423, 426, 430, 431, 434, 434, 434, 439, 442, 443, 446, 454, 455, 457, 458, 461, 462, 468, 476, 478, 481, 482, 483, 483, 485, 485, 487, 487, 490, 491, 491, 494, 494, 496, 497, 500, 503, 507, 507, 509, 512, 513, 514, 515, 516, 517, 519, 533, 534, 538, 546, 549, 552, 552, 559, 562, 565, 569, 572, 574, 577, 577, 581, 581, 582, 584, 590, 593, 599, 606, 606, 607, 609, 610, 614, 615, 615, 616, 617, 617, 620, 624, 625, 625, 626, 631, 631, 632, 632, 633, 639, 644, 647, 660, 663, 668, 671, 673, 678, 678, 688, 689, 690, 693, 701, 705, 711, 720, 721, 725, 725, 731, 733, 738, 743, 747, 749, 749, 751, 755, 757, 770, 770, 771, 772, 772, 775, 775, 777, 779, 788, 791, 798, 798, 799, 801, 803, 803, 805, 808, 808, 809, 813, 813, 816, 817, 821, 822, 824, 827, 827, 836, 837, 839, 849, 850, 862, 862, 863, 864, 864, 869, 883, 886, 888, 888, 889, 889, 893, 894, 894, 895, 896, 897, 907, 909, 910, 910, 916, 917, 926, 937, 960, 962, 970, 977, 979, 981, 981, 983, 984, 987, 988, 993, 995]\n"
     ]
    }
   ],
   "source": [
    "# 12. sample\n",
    "print(\"\\n### 12. sample ###\")\n",
    "print(\"Description: Return a sampled subset of this RDD.\")\n",
    "\n",
    "# 01 Example: Sample 50% of the elements without replacement\n",
    "simple_sample = rdd.sample(False, 0.5).collect()\n",
    "print(\"01 sample example (50% sample):\", simple_sample)\n",
    "\n",
    "# example_Example: Sample 30% of the elements with replacement\n",
    "example__sample = large_data.sample(True, 0.3).collect()\n",
    "print(\"example_ sample example (30% sample with replacement):\", example__sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e8acd617-5899-4606-9d36-be2dea3a0ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `randomSplit` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388b84f0-7074-4088-aeed-fc1d74538960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 13. randomSplit ###\nDescription: Randomly splits this RDD with the provided weights.\n01 randomSplit example (70% and 30%): [[1, 2, 3, 5], [4]]\nComplex randomSplit example (50%, 30%, and 20%): [477, 326, 197]\n"
     ]
    }
   ],
   "source": [
    "# 13. randomSplit\n",
    "print(\"\\n### 13. randomSplit ###\")\n",
    "print(\"Description: Randomly splits this RDD with the provided weights.\")\n",
    "\n",
    "# 01 Example: Split into two parts with weights 0.7 and 0.3\n",
    "simple_randomSplit = rdd.randomSplit([0.7, 0.3])\n",
    "print(\"01 randomSplit example (70% and 30%):\", [part.collect() for part in simple_randomSplit])\n",
    "\n",
    "# example_Example: Split a large dataset into three parts with weights 0.5, 0.3, and 0.2\n",
    "example__randomSplit = large_data.randomSplit([0.5, 0.3, 0.2])\n",
    "print(\"example_ randomSplit example (50%, 30%, and 20%):\", [part.count() for part in example__randomSplit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce06a5b-4c9e-4dae-8811-35c11fc7edbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `union` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0624fb86-4f3d-4afb-acc4-38a982879acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 14. union ###\nDescription: Return the union of this RDD and another one.\n01 union example (union of two RDDs): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nComplex union example (union of multiple RDDs): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# 14. union\n",
    "print(\"\\n### 14. union ###\")\n",
    "print(\"Description: Return the union of this RDD and another one.\")\n",
    "\n",
    "# 01 Example: Union two RDDs\n",
    "rdd2 = sc.parallelize([6, 7, 8, 9, 10])\n",
    "simple_union = rdd.union(rdd2).collect()\n",
    "print(\"01 union example (union of two RDDs):\", simple_union)\n",
    "\n",
    "# example_Example: Union multiple RDDs\n",
    "rdd3 = sc.parallelize([11, 12, 13])\n",
    "example__union = rdd.union(rdd2).union(rdd3).collect()\n",
    "print(\"example_ union example (union of multiple RDDs):\", example__union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f4b430-8307-4616-96d8-31e8d4edf69b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `intersection` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa65a5d-e070-4019-b169-a6ee6e5838a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 15. intersection ###\nDescription: Return the intersection of this RDD and another one.\n01 intersection example (intersection of two RDDs): []\nComplex intersection example (intersection of large datasets): [160, 176, 192, 161, 177, 193, 162, 178, 194, 163, 179, 195, 164, 180, 196, 165, 181, 197, 150, 166, 182, 198, 151, 167, 183, 199, 152, 168, 184, 153, 169, 185, 154, 170, 186, 155, 171, 187, 156, 172, 188, 157, 173, 189, 158, 174, 190, 159, 175, 191]\n"
     ]
    }
   ],
   "source": [
    "# 15. intersection\n",
    "print(\"\\n### 15. intersection ###\")\n",
    "print(\"Description: Return the intersection of this RDD and another one.\")\n",
    "\n",
    "# 01 Example: Intersection of two RDDs\n",
    "simple_intersection = rdd.intersection(rdd2).collect()\n",
    "print(\"01 intersection example (intersection of two RDDs):\", simple_intersection)\n",
    "\n",
    "# example_Example: Intersection of large datasets\n",
    "large_rdd1 = sc.parallelize(range(100, 200))\n",
    "large_rdd2 = sc.parallelize(range(150, 250))\n",
    "example__intersection = large_rdd1.intersection(large_rdd2).collect()\n",
    "print(\"example_ intersection example (intersection of large datasets):\", example__intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a343ce90-7a52-4727-8b2b-f921ed964917",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `subtract` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c77130-3e7c-4641-b3fd-060f834caafe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 16. subtract ###\nDescription: Return an RDD with the elements from this that are not in other.\n01 subtract example (subtract elements): [3, 4, 5]\nComplex subtract example (subtract elements from large dataset): [18, 36, 54, 72, 90, 108, 126, 144, 162, 180, 198, 216, 234, 252, 270, 288, 306, 324, 342, 360, 378, 396, 414, 432, 450, 468, 486, 504, 522, 540, 558, 576, 594, 612, 630, 648, 666, 684, 702, 720, 738, 756, 774, 792, 810, 828, 846, 864, 882, 900, 918, 936, 954, 972, 990, 19, 37, 55, 73, 91, 109, 127, 145, 163, 181, 199, 217, 235, 253, 271, 289, 307, 325, 343, 361, 379, 397, 415, 433, 451, 469, 487, 505, 523, 541, 559, 577, 595, 613, 631, 649, 667, 685, 703, 721, 739, 757, 775, 793, 811, 829, 847, 865, 883, 901, 919, 937, 955, 973, 991, 20, 38, 56, 74, 92, 110, 128, 146, 164, 182, 200, 218, 236, 254, 272, 290, 308, 326, 344, 362, 380, 398, 416, 434, 452, 470, 488, 506, 524, 542, 560, 578, 596, 614, 632, 650, 668, 686, 704, 722, 740, 758, 776, 794, 812, 830, 848, 866, 884, 902, 920, 938, 956, 974, 992, 21, 39, 57, 75, 93, 111, 129, 147, 165, 183, 201, 219, 237, 255, 273, 291, 309, 327, 345, 363, 381, 399, 417, 435, 453, 471, 489, 507, 525, 543, 561, 579, 597, 615, 633, 651, 669, 687, 705, 723, 741, 759, 777, 795, 813, 831, 849, 867, 885, 903, 921, 939, 957, 975, 993, 22, 40, 58, 76, 94, 112, 130, 148, 166, 184, 202, 220, 238, 256, 274, 292, 310, 328, 346, 364, 382, 400, 418, 436, 454, 472, 490, 508, 526, 544, 562, 580, 598, 616, 634, 652, 670, 688, 706, 724, 742, 760, 778, 796, 814, 832, 850, 868, 886, 904, 922, 940, 958, 976, 994, 23, 41, 59, 77, 95, 113, 131, 149, 167, 185, 203, 221, 239, 257, 275, 293, 311, 329, 347, 365, 383, 401, 419, 437, 455, 473, 491, 509, 527, 545, 563, 581, 599, 617, 635, 653, 671, 689, 707, 725, 743, 761, 779, 797, 815, 833, 851, 869, 887, 905, 923, 941, 959, 977, 995, 24, 42, 60, 78, 96, 114, 132, 150, 168, 186, 204, 222, 240, 258, 276, 294, 312, 330, 348, 366, 384, 402, 420, 438, 456, 474, 492, 510, 528, 546, 564, 582, 600, 618, 636, 654, 672, 690, 708, 726, 744, 762, 780, 798, 816, 834, 852, 870, 888, 906, 924, 942, 960, 978, 996, 25, 43, 61, 79, 97, 115, 133, 151, 169, 187, 205, 223, 241, 259, 277, 295, 313, 331, 349, 367, 385, 403, 421, 439, 457, 475, 493, 511, 529, 547, 565, 583, 601, 619, 637, 655, 673, 691, 709, 727, 745, 763, 781, 799, 817, 835, 853, 871, 889, 907, 925, 943, 961, 979, 997, 26, 44, 62, 80, 98, 116, 134, 152, 170, 188, 206, 224, 242, 260, 278, 296, 314, 332, 350, 368, 386, 404, 422, 440, 458, 476, 494, 512, 530, 548, 566, 584, 602, 620, 638, 656, 674, 692, 710, 728, 746, 764, 782, 800, 818, 836, 854, 872, 890, 908, 926, 944, 962, 980, 998, 27, 45, 63, 81, 99, 117, 135, 153, 171, 189, 207, 225, 243, 261, 279, 297, 315, 333, 351, 369, 387, 405, 423, 441, 459, 477, 495, 513, 531, 549, 567, 585, 603, 621, 639, 657, 675, 693, 711, 729, 747, 765, 783, 801, 819, 837, 855, 873, 891, 909, 927, 945, 963, 981, 999, 10, 28, 46, 64, 82, 100, 118, 136, 154, 172, 190, 208, 226, 244, 262, 280, 298, 316, 334, 352, 370, 388, 406, 424, 442, 460, 478, 496, 514, 532, 550, 568, 586, 604, 622, 640, 658, 676, 694, 712, 730, 748, 766, 784, 802, 820, 838, 856, 874, 892, 910, 928, 946, 964, 982, 11, 29, 47, 65, 83, 101, 119, 137, 155, 173, 191, 209, 227, 245, 263, 281, 299, 317, 335, 353, 371, 389, 407, 425, 443, 461, 479, 497, 515, 533, 551, 569, 587, 605, 623, 641, 659, 677, 695, 713, 731, 749, 767, 785, 803, 821, 839, 857, 875, 893, 911, 929, 947, 965, 983, 12, 30, 48, 66, 84, 102, 120, 138, 156, 174, 192, 210, 228, 246, 264, 282, 300, 318, 336, 354, 372, 390, 408, 426, 444, 462, 480, 498, 516, 534, 552, 570, 588, 606, 624, 642, 660, 678, 696, 714, 732, 750, 768, 786, 804, 822, 840, 858, 876, 894, 912, 930, 948, 966, 984, 13, 31, 49, 67, 85, 103, 121, 139, 157, 175, 193, 211, 229, 247, 265, 283, 301, 319, 337, 355, 373, 391, 409, 427, 445, 463, 481, 499, 517, 535, 553, 571, 589, 607, 625, 643, 661, 679, 697, 715, 733, 751, 769, 787, 805, 823, 841, 859, 877, 895, 913, 931, 949, 967, 985, 14, 32, 50, 68, 86, 104, 122, 140, 158, 176, 194, 212, 230, 248, 266, 284, 302, 320, 338, 356, 374, 392, 410, 428, 446, 464, 482, 500, 518, 536, 554, 572, 590, 608, 626, 644, 662, 680, 698, 716, 734, 752, 770, 788, 806, 824, 842, 860, 878, 896, 914, 932, 950, 968, 986, 15, 33, 51, 69, 87, 105, 123, 141, 159, 177, 195, 213, 231, 249, 267, 285, 303, 321, 339, 357, 375, 393, 411, 429, 447, 465, 483, 501, 519, 537, 555, 573, 591, 609, 627, 645, 663, 681, 699, 717, 735, 753, 771, 789, 807, 825, 843, 861, 879, 897, 915, 933, 951, 969, 987, 16, 34, 52, 70, 88, 106, 124, 142, 160, 178, 196, 214, 232, 250, 268, 286, 304, 322, 340, 358, 376, 394, 412, 430, 448, 466, 484, 502, 520, 538, 556, 574, 592, 610, 628, 646, 664, 682, 700, 718, 736, 754, 772, 790, 808, 826, 844, 862, 880, 898, 916, 934, 952, 970, 988, 17, 35, 53, 71, 89, 107, 125, 143, 161, 179, 197, 215, 233, 251, 269, 287, 305, 323, 341, 359, 377, 395, 413, 431, 449, 467, 485, 503, 521, 539, 557, 575, 593, 611, 629, 647, 665, 683, 701, 719, 737, 755, 773, 791, 809, 827, 845, 863, 881, 899, 917, 935, 953, 971, 989]\n"
     ]
    }
   ],
   "source": [
    "# 16. subtract\n",
    "print(\"\\n### 16. subtract ###\")\n",
    "print(\"Description: Return an RDD with the elements from this that are not in other.\")\n",
    "\n",
    "# 01 Example: Subtract elements of another RDD\n",
    "simple_subtract = rdd.subtract(sc.parallelize([1, 2])).collect()\n",
    "print(\"01 subtract example (subtract elements):\", simple_subtract)\n",
    "\n",
    "# example_Example: Subtract elements from a large dataset\n",
    "example__subtract = large_data.subtract(sc.parallelize(range(10))).collect()\n",
    "print(\"example_ subtract example (subtract elements from large dataset):\", example__subtract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad5c938-08ba-4816-a1d3-32a159744ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `groupBy` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60f70e7-d83e-409d-9f08-edd02a6cce35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 17. groupBy ###\nDescription: Return an RDD of grouped items. Each group consists of a key and a sequence of elements mapping to that key.\n01 groupBy example (group by even and odd): [(0, [2, 4]), (1, [1, 3, 5])]\nComplex groupBy example (group words by first letter): [('e', ['elephant']), ('c', ['cat', 'cat']), ('d', ['dog', 'dog'])]\n"
     ]
    }
   ],
   "source": [
    "# 17. groupBy\n",
    "print(\"\\n### 17. groupBy ###\")\n",
    "print(\"Description: Return an RDD of grouped items. Each group consists of a key and a sequence of elements mapping to that key.\")\n",
    "\n",
    "# 01 Example: Group numbers by even and odd\n",
    "simple_groupBy = rdd.groupBy(lambda x: x % 2).mapValues(list).collect()\n",
    "print(\"01 groupBy example (group by even and odd):\", simple_groupBy)\n",
    "\n",
    "# example_Example: Group words by their first letter\n",
    "example__groupBy = words_rdd.groupBy(lambda x: x[0]).mapValues(list).collect()\n",
    "print(\"example_ groupBy example (group words by first letter):\", example__groupBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245cfa50-3218-4d05-a037-7f97a7340a40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `cartesian` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a138c37d-4f3e-4234-8341-18c920a55be0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 18. cartesian ###\nDescription: Return the Cartesian product of this RDD and another one.\n01 cartesian example (Cartesian product): [(1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10)]\nComplex cartesian example (Cartesian product of large datasets): [(100, 200), (100, 201), (100, 202), (100, 203), (100, 204), (101, 200), (101, 201), (101, 202), (101, 203), (101, 204), (102, 200), (102, 201), (102, 202), (102, 203), (102, 204), (103, 200), (103, 201), (103, 202), (103, 203), (103, 204), (104, 200), (104, 201), (104, 202), (104, 203), (104, 204)]\n"
     ]
    }
   ],
   "source": [
    "# 18. cartesian\n",
    "print(\"\\n### 18. cartesian ###\")\n",
    "print(\"Description: Return the Cartesian product of this RDD and another one.\")\n",
    "\n",
    "# 01 Example: Cartesian product of two RDDs\n",
    "simple_cartesian = rdd.cartesian(rdd2).collect()\n",
    "print(\"01 cartesian example (Cartesian product):\", simple_cartesian)\n",
    "\n",
    "# example_Example: Cartesian product of large datasets\n",
    "large_rdd1 = sc.parallelize(range(100, 105))\n",
    "large_rdd2 = sc.parallelize(range(200, 205))\n",
    "example__cartesian = large_rdd1.cartesian(large_rdd2).collect()\n",
    "print(\"example_ cartesian example (Cartesian product of large datasets):\", example__cartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2ac481-74cd-4354-afbb-3dae77a3e37b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `pipe` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e1117c-138c-4845-af87-dcbbca473af4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 19. pipe ###\nDescription: Return an RDD created by piping elements to a forked external process.\n01 pipe example (pipe to 'cat' command): ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "# 19. pipe\n",
    "print(\"\\n### 19. pipe ###\")\n",
    "print(\"Description: Return an RDD created by piping elements to a forked external process.\")\n",
    "\n",
    "# 01 Example: Pipe elements to 'cat' command\n",
    "simple_pipe = rdd.pipe('cat').collect()\n",
    "print(\"01 pipe example (pipe to 'cat' command):\", simple_pipe)\n",
    "\n",
    "# example_Example: Pipe elements through a shell script\n",
    "# Note: For demonstration purposes, assuming a script 'echo.sh' that echoes input\n",
    "#example__pipe = rdd.pipe('./echo.sh').collect()\n",
    "#print(\"example_ pipe example (pipe through 'echo.sh' script):\", example__pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8ab7f9-63f9-44b4-a499-232d3655f78b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `zip` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250b13ab-c2b7-4554-ae9d-5f2dad969edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 20. zip ###\nDescription: Zips this RDD with another one, returning key-value pairs with the first element in each RDD, second element in each RDD, etc.\n01 zip example (zip two RDDs): [(1, 6), (2, 7), (3, 8), (4, 9), (5, 10)]\nComplex zip example (zip large datasets): [(1000, 2000), (1001, 2001), (1002, 2002), (1003, 2003), (1004, 2004)]\n"
     ]
    }
   ],
   "source": [
    "# 20. zip\n",
    "print(\"\\n### 20. zip ###\")\n",
    "print(\"Description: Zips this RDD with another one, returning key-value pairs with the first element in each RDD, second element in each RDD, etc.\")\n",
    "\n",
    "# 01 Example: Zip two RDDs\n",
    "simple_zip = rdd.zip(rdd2).collect()\n",
    "print(\"01 zip example (zip two RDDs):\", simple_zip)\n",
    "\n",
    "# example_Example: Zip large datasets\n",
    "large_rdd3 = sc.parallelize(range(1000, 1005))\n",
    "large_rdd4 = sc.parallelize(range(2000, 2005))\n",
    "example__zip = large_rdd3.zip(large_rdd4).collect()\n",
    "print(\"example_ zip example (zip large datasets):\", example__zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9feba7a7-a900-44de-83c6-172c57b320cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `zipPartitions` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7513d422-9b6a-4900-8d25-004c02aa51b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 21. zipPartitions ###\nDescription: Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-721154795914227>:6\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDescription: Zip this RDD\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 01 Example: Zip partitions of two RDDs and sum their elements\u001B[39;00m\n",
       "\u001B[0;32m----> 6\u001B[0m simple_zipPartitions \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mzipPartitions(rdd2, \u001B[38;5;28;01mlambda\u001B[39;00m x, y: (a \u001B[38;5;241m+\u001B[39m b \u001B[38;5;28;01mfor\u001B[39;00m a, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(x, y)))\u001B[38;5;241m.\u001B[39mcollect()\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01 zipPartitions example (sum elements in partitions):\u001B[39m\u001B[38;5;124m\"\u001B[39m, simple_zipPartitions)\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# 02 Example: Zip partitions of large datasets and multiply their elements\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'RDD' object has no attribute 'zipPartitions'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-721154795914227>:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDescription: Zip this RDD\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 01 Example: Zip partitions of two RDDs and sum their elements\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m simple_zipPartitions \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mzipPartitions(rdd2, \u001B[38;5;28;01mlambda\u001B[39;00m x, y: (a \u001B[38;5;241m+\u001B[39m b \u001B[38;5;28;01mfor\u001B[39;00m a, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(x, y)))\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01 zipPartitions example (sum elements in partitions):\u001B[39m\u001B[38;5;124m\"\u001B[39m, simple_zipPartitions)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# 02 Example: Zip partitions of large datasets and multiply their elements\u001B[39;00m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RDD' object has no attribute 'zipPartitions'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'RDD' object has no attribute 'zipPartitions'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 21. zipPartitions\n",
    "print(\"\\n### 21. zipPartitions ###\")\n",
    "print(\"Description: Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by applying a function to the zipped partitions.\")\n",
    "\n",
    "# 01 Example: Zip partitions of two RDDs and sum their elements\n",
    "simple_zipPartitions = rdd.zipPartitions(rdd2, lambda x, y: (a + b for a, b in zip(x, y))).collect()\n",
    "print(\"01 zipPartitions example (sum elements in partitions):\", simple_zipPartitions)\n",
    "\n",
    "# example_Example: Zip partitions of large datasets and multiply their elements\n",
    "example__zipPartitions = large_rdd3.zipPartitions(large_rdd4, lambda x, y: (a * b for a, b in zip(x, y))).collect()\n",
    "print(\"example_ zipPartitions example (multiply elements in partitions):\", example__zipPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58849e6f-9134-43b9-93b8-8c873245b57a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `mapPartitions` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8126c8c7-af57-4f06-a7a4-2fc1909ad945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 22. mapPartitions\n",
    "print(\"\\n### 22. mapPartitions ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to each partition of this RDD.\")\n",
    "\n",
    "# 01 Example: Sum elements within each partition\n",
    "simple_mapPartitions = rdd.mapPartitions(lambda x: [sum(x)]).collect()\n",
    "print(\"01 mapPartitions example (sum of elements in each partition):\", simple_mapPartitions)\n",
    "\n",
    "# example_Example: Find the maximum element within each partition\n",
    "example__mapPartitions = large_data.mapPartitions(lambda x: [max(x)]).collect()\n",
    "print(\"example_ mapPartitions example (max element in each partition):\", example__mapPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a45624-94bc-4c78-8cd6-80d03eb30f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `mapPartitionsWithIndex` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cb89c8-0558-42d1-bbda-3e40b16fc185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 23. mapPartitionsWithIndex\n",
    "print(\"\\n### 23. mapPartitionsWithIndex ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.\")\n",
    "\n",
    "# 01 Example: Add partition index to elements\n",
    "simple_mapPartitionsWithIndex = rdd.mapPartitionsWithIndex(lambda idx, x: [(idx, e) for e in x]).collect()\n",
    "print(\"01 mapPartitionsWithIndex example (add partition index):\", simple_mapPartitionsWithIndex)\n",
    "\n",
    "# example_Example: Find the partition index and max element within each partition\n",
    "example__mapPartitionsWithIndex = large_data.mapPartitionsWithIndex(lambda idx, x: [(idx, max(x))]).collect()\n",
    "print(\"example_ mapPartitionsWithIndex example (partition index and max element):\", example__mapPartitionsWithIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aabf9f6-42f4-44d0-b9a1-4f7316f857e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49f2a96-024f-4695-919a-7a1552bc400e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `collect` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9dec98-3be1-42bf-98dc-98cb37c98886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 24. collect\n",
    "print(\"\\n### 24. collect ###\")\n",
    "print(\"Description: Return an array that contains all of the elements in this RDD.\")\n",
    "\n",
    "# 01 Example: Collect elements of the RDD\n",
    "simple_collect = rdd.collect()\n",
    "print(\"01 collect example (collect elements):\", simple_collect)\n",
    "\n",
    "# example_Example: Collect elements of a large dataset\n",
    "example__collect = large_data.collect()\n",
    "print(\"example_ collect example (collect large dataset):\", example__collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d11e0-2f0e-489d-ad72-63e94defdeb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `count` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc2dc29-31b8-4c87-aaef-79552de2bb97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 25. count\n",
    "print(\"\\n### 25. count ###\")\n",
    "print(\"Description: Return the number of elements in the RDD.\")\n",
    "\n",
    "# 01 Example: Count the number of elements\n",
    "simple_count = rdd.count()\n",
    "print(\"01 count example (number of elements):\", simple_count)\n",
    "\n",
    "# example_Example: Count the number of distinct words\n",
    "example__count = words_rdd.distinct().count()\n",
    "print(\"example_ count example (number of unique words):\", example__count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736df68b-84c9-40c9-a272-0cf254e4cc2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Take` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e09755f-b3ab-494d-855c-82cbc11490e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 26. take\n",
    "print(\"\\n### 26. take ###\")\n",
    "print(\"Description: Take the first num elements of the RDD.\")\n",
    "\n",
    "# 01 Example: Take the first 3 elements\n",
    "simple_take = rdd.take(3)\n",
    "print(\"01 take example (first 3 elements):\", simple_take)\n",
    "\n",
    "# example_Example: Take the first 10 elements from a large dataset\n",
    "example__take = large_data.take(10)\n",
    "print(\"example_ take example (first 10 elements):\", example__take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2fce10-4de7-4c4c-8ae1-2694ab1426f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `takeSample` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f59b6b5-ed00-4797-84af-1ef280f71116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 27. takeSample\n",
    "print(\"\\n### 27. takeSample ###\")\n",
    "print(\"Description: Return a fixed-size sampled subset of this RDD.\")\n",
    "\n",
    "# 01 Example: Take 3 samples without replacement\n",
    "simple_takeSample = rdd.takeSample(False, 3)\n",
    "print(\"01 takeSample example (3 samples):\", simple_takeSample)\n",
    "\n",
    "# example_Example: Take 5 samples with replacement\n",
    "example__takeSample = large_data.takeSample(True, 5)\n",
    "print(\"example_ takeSample example (5 samples with replacement):\", example__takeSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3856df38-d887-4fdd-b939-fb20e0dcb8e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `foreach` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6f5eeb-c0ab-425a-8ffc-41473e0a6941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 28. foreach\n",
    "print(\"\\n### 28. foreach ###\")\n",
    "print(\"Description: Applies a function f to all elements of this RDD.\")\n",
    "\n",
    "# 01 Example: Print each element\n",
    "print(\"01 foreach example (print each element):\")\n",
    "rdd.foreach(lambda x: print(x))\n",
    "\n",
    "# example_Example: Save each element to a file\n",
    "# Note: For demonstration purposes, assuming a function save_to_file is defined\n",
    "def save_to_file(x):\n",
    "    with open('/tmp/output.txt', 'a') as f:\n",
    "        f.write(f\"{x}\\n\")\n",
    "\n",
    "rdd.foreach(save_to_file)\n",
    "print(\"example_ foreach example (save each element to a file): Check /tmp/output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf623f7-15b6-4273-a1c7-74e24ccbafed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `foreachPartition` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f27732c-e413-45d8-8b29-cf99c2dd545d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 29. foreachPartition\n",
    "print(\"\\n### 29. foreachPartition ###\")\n",
    "print(\"Description: Applies a function f to each partition of this RDD.\")\n",
    "\n",
    "# 01 Example: Print each partition\n",
    "print(\"01 foreachPartition example (print each partition):\")\n",
    "rdd.foreachPartition(lambda x: print(list(x)))\n",
    "\n",
    "# example_Example: Save each partition to a file\n",
    "# Note: For demonstration purposes, assuming a function save_partition_to_file is defined\n",
    "def save_partition_to_file(iterator):\n",
    "    with open('/tmp/partition_output.txt', 'a') as f:\n",
    "        for record in iterator:\n",
    "            f.write(f\"{record}\\n\")\n",
    "\n",
    "rdd.foreachPartition(save_partition_to_file)\n",
    "print(\"example_ foreachPartition example (save each partition to a file): Check /tmp/partition_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fd50c4-8cbb-4f79-8ad4-a6f698e658e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `aggregate` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86f28bf-8e8d-472c-a9b0-cc397fcc8323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 30. aggregate\n",
    "print(\"\\n### 30. aggregate ###\")\n",
    "print(\"Description: Aggregate the elements of each partition, and then the results for all the partitions, using given combine functions and a neutral 'zero value'.\")\n",
    "\n",
    "# 01 Example: Aggregate sum of elements\n",
    "simple_aggregate = rdd.aggregate(0, lambda x, y: x + y, lambda x, y: x + y)\n",
    "print(\"01 aggregate example (sum of elements):\", simple_aggregate)\n",
    "\n",
    "# example_Example: Aggregate min and max of elements\n",
    "example__aggregate = large_data.aggregate((float('inf'), float('-inf')),\n",
    "                                         lambda acc, val: (min(acc[0], val), max(acc[1], val)),\n",
    "                                         lambda acc1, acc2: (min(acc1[0], acc2[0]), max(acc1[1], acc2[1])))\n",
    "print(\"example_ aggregate example (min and max of elements):\", example__aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e80f481-0553-446b-91d8-c9399135d285",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `fold` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55853fab-cf94-4847-ae9a-432a61064f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 31. fold\n",
    "print(\"\\n### 31. fold ###\")\n",
    "print(\"Description: Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral 'zero value'.\")\n",
    "\n",
    "# 01 Example: Fold sum of elements\n",
    "simple_fold = rdd.fold(0, lambda x, y: x + y)\n",
    "print(\"01 fold example (sum of elements):\", simple_fold)\n",
    "\n",
    "# example_Example: Fold min and max of elements\n",
    "example__fold = large_data.fold((float('inf'), float('-inf')),\n",
    "                               lambda acc, val: (min(acc[0], val), max(acc[1], val)))\n",
    "print(\"example_ fold example (min and max of elements):\", example__fold)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "rdd_operations",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
