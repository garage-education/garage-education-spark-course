{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a14ee98-580f-4e37-8af3-bfa359f7d443",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `repartition` Vs. `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d68b7268-15e9-435e-b383-90c3313702cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random log entry\n",
    "def generate_log_entry():\n",
    "    user_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "    action = random.choice([\"login\", \"logout\", \"purchase\", \"click\", \"view\"])\n",
    "    item_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
    "    timestamp = (datetime.now() - timedelta(seconds=random.randint(0, 2592000))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return (user_id, action, item_id, timestamp)\n",
    "\n",
    "# Generate synthetic data\n",
    "log_entries = [generate_log_entry() for _ in range(1000000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"action\", \"item_id\", \"timestamp\"]\n",
    "log_df = spark.createDataFrame(log_entries, columns)\n",
    "\n",
    "# Show sample data\n",
    "log_df.show(10, truncate=False)\n",
    "\n",
    "# Save to a CSV file in the DBFS (Databricks File System)\n",
    "log_df.write.csv(\"/tmp/user_logs\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048b33b1-4815-42aa-8987-a0bbe9ce85e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `repartition` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50e53834-a606-4802-8c74-92a9d6f9f945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10. repartition\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L480\n",
    "print(\"\\n### 10. repartition ###\")\n",
    "print(\"Description: Return a new RDD that has exactly numPartitions partitions.\")\n",
    "\n",
    "logs_rdd = sc.textFile(\"/tmp/user_logs\")\n",
    "\n",
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# Repartition to 200 partitions\n",
    "repartitioned_rdd = logs_rdd.repartition(100)\n",
    "new_partitions = repartitioned_rdd.getNumPartitions()\n",
    "print(f\"New Partitions after Repartition: {new_partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20d17a0e-433e-4a3a-8da6-28f702af5a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `coalesce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5aa61cb-88ab-47cc-a8bf-f1c61b882a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11. coalesce\n",
    "print(\"\\n### 11. coalesce ###\")\n",
    "print(\"Description: Return a new RDD that is reduced into numPartitions partitions.\")\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L506\n",
    "#https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala\n",
    "# Initial number of partitions\n",
    "initial_partitions = logs_rdd.getNumPartitions()\n",
    "print(f\"Initial Partitions: {initial_partitions}\")\n",
    "\n",
    "# Coalesce to 4 partitions\n",
    "coalesced_rdd_1 = logs_rdd.coalesce(2)\n",
    "new_partitions_1 = coalesced_rdd_1.getNumPartitions()\n",
    "print(f\"new_partitions_1 after Coalesce: {new_partitions_1}\")\n",
    "\n",
    "# Coalesce to 50 partitions\n",
    "coalesced_rdd_2 = logs_rdd.coalesce(10)\n",
    "new_partitions_2 = coalesced_rdd_2.getNumPartitions()\n",
    "print(f\"new_partitions_2 after Coalesce: {new_partitions_2}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "repartition-vs-coalesce",
   "widgets": {}
  },
  "jupytext": {
   "formats": "ipynb,md"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
