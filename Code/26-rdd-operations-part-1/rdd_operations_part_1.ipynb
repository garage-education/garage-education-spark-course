{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccf76f1-e7b6-486d-bba3-9ed9748d8d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a88255-f6c0-4972-b395-6bccb46e4fed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bea342-ed95-4387-9cd0-d41be8b0f493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Map` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b664677a-d399-4a30-9e3a-379ec91939fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. map ###\nDescription: Return a new RDD by applying a function to all elements of this RDD.\n01 map example (multiply by 2): [2, 4, 6, 8, 10]\nexample_map example (word count in sentences): [2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "# 1. map\n",
    "print(\"### 1. map ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD.\")\n",
    "\n",
    "# Example 1: Multiply each element by 2\n",
    "simple_map = rdd.map(lambda x: x * 2).collect()\n",
    "print(\"01 map example (multiply by 2):\", simple_map)\n",
    "\n",
    "# Example 2: Extract the length of each word in a list of sentences\n",
    "sentences = [\"Hello world\", \"Apache Spark\", \"RDD transformations Wide Vs Narrow Spark\"]\n",
    "# Hello World => split (\" \") => [(0)-> Hello, (1) -> World]\n",
    "sentence_rdd = sc.parallelize(sentences)\n",
    "words_map = sentence_rdd.map(lambda sentence: len(sentence.split(\" \"))).collect()\n",
    "print(\"example_map example (word count in sentences):\", words_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1c213a-1574-43e1-8251-8836420c07d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Filter` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00eaeb0a-b58d-43f9-947f-e2d47787cb60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 2. filter ###\nDescription: Return a new RDD containing only the elements that satisfy a predicate.\n01 filter example (even numbers): [2, 4]\nexample_ filter example (sentences with 'Spark'): ['Apache Spark', 'RDD transformations Wide Vs Narrow Spark']\n"
     ]
    }
   ],
   "source": [
    "# 2. filter\n",
    "print(\"\\n### 2. filter ###\")\n",
    "print(\"Description: Return a new RDD containing only the elements that satisfy a predicate.\")\n",
    "\n",
    "# 01 Example: Filter out even numbers\n",
    "simple_filter = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "print(\"01 filter example (even numbers):\", simple_filter)\n",
    "\n",
    "# example_Example: Filter sentences containing the word 'Spark'\n",
    "words_filter = sentence_rdd.filter(lambda sentence: \"Spark\" in sentence).collect()\n",
    "print(\"example_ filter example (sentences with 'Spark'):\", words_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438692c8-0dd5-4bc0-89d8-460cdec835a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `FlatMap` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae10aa1-037a-4a4f-8d7d-0201a8837e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 3. flatMap ###\nDescription: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\n01 sentences_mapped: [['Hello', 'world'], ['Apache', 'Spark'], ['RDD', 'transformations', 'Wide', 'Vs', 'Narrow', 'Spark']]\n02 flatMap example (split sentences into words): ['Hello', 'world', 'Apache', 'Spark', 'RDD', 'transformations', 'Wide', 'Vs', 'Narrow', 'Spark']\nflatten_list  flatMap example (flatten list of lists): [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# 3. flatMap\n",
    "print(\"\\n### 3. flatMap ###\")\n",
    "print(\"Description: Return a new RDD by applying a function to all elements of this RDD and then flattening the results.\")\n",
    "\n",
    "# 01 Example: Split sentences into words\n",
    "sentences_mapped = sentence_rdd.map(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(\"01 sentences_mapped:\", sentences_mapped)\n",
    "\n",
    "simple_flatMap = sentence_rdd.flatMap(lambda sentence: sentence.split(\" \")).collect()\n",
    "print(\"02 flatMap example (split sentences into words):\", simple_flatMap)\n",
    "\n",
    "# example_Example: Flatten a list of lists\n",
    "nested_lists = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "nested_rdd = sc.parallelize(nested_lists)\n",
    "flatten_list = nested_rdd.flatMap(lambda x: x).collect()\n",
    "print(\"flatten_list  flatMap example (flatten list of lists):\", flatten_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0c9fa6-6ccf-465f-a7f5-b59bbc379577",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `Reduce` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f23341-169c-45e0-84a6-d9a030e27f20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 4. reduce ###\nDescription: Reduces the elements of this RDD using the specified commutative and associative binary operator.\n01 reduce example (sum of elements): 15\nreduce example (longest word): hippopotamus\n"
     ]
    }
   ],
   "source": [
    "# 4. reduce\n",
    "print(\"\\n### 4. reduce ###\")\n",
    "print(\"Description: Reduces the elements of this RDD using the specified commutative and associative binary operator.\")\n",
    "\n",
    "# 01 Example: Sum of elements\n",
    "simple_reduce = rdd.reduce(lambda x, y: x + y)\n",
    "print(\"01 reduce example (sum of elements):\", simple_reduce)\n",
    "\n",
    "# example_Example: Find the longest word in a list of words\n",
    "words = [\"cat\", \"elephant\", \"rat\", \"hippopotamus\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "words_rdd_reduced = words_rdd.reduce(lambda x, y: x if len(x) > len(y) else y)\n",
    "print(\"reduce example (longest word):\", words_rdd_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710eaaca-e6f8-44e4-8dc9-1b52f1284db8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `groupByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3989c5d-38c5-4685-af57-8264dbeb83f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 5. groupByKey ###\nDescription: Group the values for each key in the RDD into a single sequence.\n01 groupByKey example (group numbers): [(1, ['a', 'ali']), (2, ['b']), (3, ['c']), (4, ['d']), (5, ['e'])]\nwords_grouped example (group words by starting letter): [('elephant', [5, 20]), ('dog', [3]), ('cat', [1]), ('car', [2]), ('deer', [4])]\n"
     ]
    }
   ],
   "source": [
    "# 5. groupByKey\n",
    "print(\"\\n### 5. groupByKey ###\")\n",
    "print(\"Description: Group the values for each key in the RDD into a single sequence.\")\n",
    "\n",
    "# 01 Example: Group numbers by even and odd\n",
    "pairs = [(1, 'a'),(1, 'ali'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "simple_groupByKey = pairs_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"01 groupByKey example (group numbers):\", simple_groupByKey)\n",
    "\n",
    "# example_Example: Group words by their starting letter\n",
    "words_pairs = [(\"cat\", 1), (\"car\", 2), (\"dog\", 3), (\"deer\", 4), (\"elephant\", 5),(\"elephant\", 20)]\n",
    "words_rdd = sc.parallelize(words_pairs)\n",
    "# mapValues(list) converts the grouped values (which are iterable) into lists.\n",
    "words_grouped = words_rdd.groupByKey().mapValues(list).collect()\n",
    "print(\"words_grouped example (group words by starting letter):\", words_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe0346e-04cd-4f97-9c9e-35bfcd876ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `reduceByKey` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c3a0324-15ec-45c0-903e-a2f722cc354b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 6. reduceByKey ###\nDescription: Merge the values for each key using an associative and commutative reduce function.\n01 reduceByKey example (sum values by key): [(1, 'a_a'), (2, 'b_b'), (3, 'c'), (4, 'd'), (5, 'e')]\nexample_ reduceByKey example (word count): [('elephant', 1), ('dog', 3), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 6. reduceByKey\n",
    "print(\"\\n### 6. reduceByKey ###\")\n",
    "print(\"Description: Merge the values for each key using an associative and commutative reduce function.\")\n",
    "pairs = [(1, 'a'),(1, '_a'), (2, 'b'), (2, '_b'), (3, 'c'), (4, 'd'), (5, 'e')]\n",
    "pairs_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# 01 Example: Sum values with the same key\n",
    "simple_reduceByKey = pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"01 reduceByKey example (sum values by key):\", simple_reduceByKey)\n",
    "\n",
    "# example_Example: Count the occurrences of each word in a list\n",
    "word_list = [\"cat\", \"cat\", \"dog\", \"elephant\", \"dog\", \"dog\"]\n",
    "word_pairs_rdd = sc.parallelize(word_list).map(lambda word: (word, 1))\n",
    "example__reduceByKey = word_pairs_rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "print(\"example_ reduceByKey example (word count):\", example__reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c024f23f-0cf9-46b7-b6dd-ddde4fdfa637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `join` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6ea909-41f4-420e-b56d-497b23d17441",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 7. join ###\nDescription: Perform an inner join of this RDD and another one.\n01 join fruits_color_join (join two RDDs): [(1, ('apple', 'red')), (2, ('banana', 'yellow'))]\njoin example (employee-department join): [(1, ('John', 'HR')), (2, ('Jane', 'Finance'))]\n"
     ]
    }
   ],
   "source": [
    "# 7. join\n",
    "print(\"\\n### 7. join ###\")\n",
    "print(\"Description: Perform an inner join of this RDD and another one.\")\n",
    "\n",
    "# 01 Example: Join two RDDs by key\n",
    "fruits = sc.parallelize([(1, \"apple\"), (2, \"banana\")])\n",
    "colors = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "fruits_color_join = fruits.join(colors).collect()\n",
    "print(\"01 join fruits_color_join (join two RDDs):\", fruits_color_join)\n",
    "\n",
    "# example_Example: Join employee data with department data\n",
    "employees = sc.parallelize([(1, \"John\"), (2, \"Jane\"), (3, \"Joe\")])\n",
    "departments = sc.parallelize([(1, \"HR\"), (2, \"Finance\")])\n",
    "employees_department_join = employees.join(departments).collect()\n",
    "print(\"join example (employee-department join):\", employees_department_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169c5a2e-a955-48f3-8e31-116ea098dc3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `cogroup` Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9add167-5a88-4fd2-a673-b84029411589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TableA:\n",
    "\n",
    "| id | value  |\n",
    "|----|--------|\n",
    "|  1 | apple  |\n",
    "|  2 | banana |\n",
    "|  3 | orange |\n",
    "\n",
    "\n",
    "TableB:\n",
    "\n",
    "| id | color  |\n",
    "|----|--------|\n",
    "|  1 | red    |\n",
    "|  2 | yellow |\n",
    "\n",
    "\n",
    "Result of cogroup:\n",
    "\n",
    "| id | value  | color  |\n",
    "|----|--------|--------|\n",
    "|  1 | apple  | red    |\n",
    "|  2 | banana | yellow |\n",
    "|  3 | orange | NULL   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7d171e-b75a-47b7-92a8-051050c1ba34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 8. cogroup ###\nDescription: Group data from two RDDs sharing the same key.\n01 cogroup example (group two RDDs): [(1, (['apple'], ['red'])), (2, (['banana'], ['yellow'])), (3, (['orange'], []))]\nexample_cogroup example (sales-targets cogroup): [('store2', ([200], [])), ('store3', ([], [250])), ('store1', ([100], [150]))]\n"
     ]
    }
   ],
   "source": [
    "# 8. cogroup\n",
    "# The cogroup function in PySpark is used to group data from two RDDs that share the same key. \n",
    "# It combines the values of matching keys from both RDDs into a tuple of lists.\n",
    "print(\"\\n### 8. cogroup ###\")\n",
    "print(\"Description: Group data from two RDDs sharing the same key.\")\n",
    "\n",
    "# 01 Example: Cogroup two RDDs\n",
    "fruits_rdd = sc.parallelize([(1, \"apple\"), (2, \"banana\"), (3, \"orange\")])\n",
    "colors_rdd = sc.parallelize([(1, \"red\"), (2, \"yellow\")])\n",
    "cogrouped_fruits_colors = fruits_rdd.cogroup(colors_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"01 cogroup example (group two RDDs):\", cogrouped_fruits_colors)\n",
    "\n",
    "\n",
    "\n",
    "# example_Example: Cogroup sales data with target data\n",
    "sales_rdd = sc.parallelize([(\"store1\", 100), (\"store2\", 200)])\n",
    "targets_rdd = sc.parallelize([(\"store1\", 150), (\"store3\", 250)])\n",
    "cogrouped_sales_targets = sales_rdd.cogroup(targets_rdd).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()\n",
    "print(\"example_cogroup example (sales-targets cogroup):\", cogrouped_sales_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4091e572-2689-4000-bdb9-4e4323626ae3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `distinct` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15c5dc53-6ef2-4cbc-886e-085d9e2e8a44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n### 9. distinct ###\nDescription: Return a new RDD containing the distinct elements in this RDD.\nexample_distinct example (unique words): ['elephant', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# 9. distinct\n",
    "print(\"\\n### 9. distinct ###\")\n",
    "print(\"Description: Return a new RDD containing the distinct elements in this RDD.\")\n",
    "\n",
    "# example_Example: Unique words from a list of words\n",
    "words = [\"cat\", \"dog\", \"cat\", \"elephant\", \"dog\"]\n",
    "words_rdd = sc.parallelize(words)\n",
    "example__distinct = words_rdd.distinct().collect()\n",
    "print(\"example_distinct example (unique words):\", example__distinct)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "rdd_operations_part_1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
