{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3186ab0c-8c5d-4bfd-8898-ae5645e1179f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Immutable RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cd2dc5-a498-43e5-b984-095b2522460a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD ID: 227\n",
      "Original RDD ID: 227\n",
      "Transformed RDD ID: 228\n",
      "Transformed RDD result: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Test Immutable RDDs\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "print(f\"Original RDD ID: {numbers_rdd.id()}\")\n",
    "print(f\"Original RDD ID: {numbers_rdd.id()}\")\n",
    "\n",
    "# # Apply a transformation: multiply each number by 2\n",
    "transformed_rdd = numbers_rdd.map(lambda x: x * 2)\n",
    "print(f\"Transformed RDD ID: {transformed_rdd.id()}\")\n",
    "\n",
    "# # Collect the results to trigger the computation\n",
    "result = transformed_rdd.collect()\n",
    "print(f\"Transformed RDD result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670e297b-4b3d-4aea-9d17-f288148936ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Original RDD ID: 221\n",
       "Original RDD ID: 221\n",
       "Original RDD ID: 221\n",
       "numbers: List[Int] = List(1, 2, 3, 4, 5)\n",
       "numbersRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[221] at parallelize at command-1340749351530580:3\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Original RDD ID: 221\nOriginal RDD ID: 221\nOriginal RDD ID: 221\nnumbers: List[Int] = List(1, 2, 3, 4, 5)\nnumbersRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[221] at parallelize at command-1340749351530580:3\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Test Immutable RDDs\n",
    "val numbers = List(1, 2, 3, 4, 5)\n",
    "val numbersRdd = sc.parallelize(numbers)\n",
    "println(s\"Original RDD ID: ${numbersRdd.id}\")\n",
    "println(s\"Original RDD ID: ${numbersRdd.id}\")\n",
    "println(s\"Original RDD ID: ${numbersRdd.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9522dbb6-548c-4bab-8410-4ef2362458d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Transformed RDD ID: 223\n",
       "Transformed RDD result: 2, 4, 6, 8, 10\n",
       "transformedRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[223] at map at command-1340749351530619:5\n",
       "result: Array[Int] = Array(2, 4, 6, 8, 10)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Transformed RDD ID: 223\nTransformed RDD result: 2, 4, 6, 8, 10\ntransformedRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[223] at map at command-1340749351530619:5\nresult: Array[Int] = Array(2, 4, 6, 8, 10)\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// numbersRdd = numbersRdd.map(x => x * 2) //OPS!!!!!!!!!!!\n",
    "\n",
    "// Apply a transformation: multiply each number by 2\n",
    "val transformedRdd = numbersRdd.map(x => x * 2)\n",
    "println(s\"Transformed RDD ID: ${transformedRdd.id}\")\n",
    "\n",
    "// Collect the results to trigger the computation\n",
    "val result = transformedRdd.collect()\n",
    "println(s\"Transformed RDD result: ${result.mkString(\", \")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c622b5-a047-423c-a449-eb04f4d89ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PartRDD -> RDD[ (integer, Part)]\n",
    "# PartSuppRDD -> RDD [ (integer, PartSupp)]\n",
    "# JoinedRDD -> RDD [(integer,(Part,PartSupp)) ]\n",
    "# Perform inner join on part and partsupp datasets\n",
    "part_joined_partsupp = part_transformed.join(partsupp_mapped)\n",
    "\n",
    "# Take the first 10 elements of the joined RDD and print them\n",
    "# for record in part_joined_partsupp.take(10):\n",
    "#     print(record)\n",
    "\n",
    "# Print the count of joined records\n",
    "print(f\"Number of joined records = {part_joined_partsupp.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad07a750-e6d9-44b3-8ae6-b84a37a7d4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Immutable DF Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96acf79-728c-43c8-8e70-bb1f29a40e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD:\n",
      "('John', 28)\n",
      "('Smith', 44)\n",
      "('Adam', 65)\n",
      "('Henry', 23)\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD\n",
    "data = [(\"John\", 28), (\"Smith\", 44), (\"Adam\", 65), (\"Henry\", 23)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Show the original RDD\n",
    "print(\"Original RDD:\")\n",
    "for row in rdd.collect():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889c7a78-18c2-4c55-851e-dc1630c278f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD ID: 316\n",
      "Original RDD ID After filter: 317\n",
      "Transformed RDD ID: 318\n",
      "Filtered RDD:\n",
      "('Smith', 44)\n",
      "('Adam', 65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Original RDD ID: {rdd.id()}\")\n",
    "\n",
    "rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "\n",
    "print(f\"Original RDD ID After filter: {rdd.id()}\")\n",
    "\n",
    "# Filter rows where the age is greater than 30\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "print(f\"Transformed RDD ID: {filtered_rdd.id()}\")\n",
    "\n",
    "# Show the transformed RDD\n",
    "print(\"Filtered RDD:\")\n",
    "for row in filtered_rdd.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5588cf-bc58-4bf0-a5a6-72c74a701416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Original RDD:\n",
       "(John,28)\n",
       "(Smith,44)\n",
       "(Adam,65)\n",
       "(Henry,23)\n",
       "Transformed RDD ID: 320\n",
       "Filtered RDD:\n",
       "(Smith,44)\n",
       "(Adam,65)\n",
       "data: Seq[(String, Int)] = List((John,28), (Smith,44), (Adam,65), (Henry,23))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[319] at parallelize at command-1340749351530616:3\n",
       "filteredRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[320] at filter at command-1340749351530616:10\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Original RDD:\n(John,28)\n(Smith,44)\n(Adam,65)\n(Henry,23)\nTransformed RDD ID: 320\nFiltered RDD:\n(Smith,44)\n(Adam,65)\ndata: Seq[(String, Int)] = List((John,28), (Smith,44), (Adam,65), (Henry,23))\nrdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[319] at parallelize at command-1340749351530616:3\nfilteredRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[320] at filter at command-1340749351530616:10\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "    // Create an RDD\n",
    "    val data = Seq((\"John\", 28), (\"Smith\", 44), (\"Adam\", 65), (\"Henry\", 23))\n",
    "    val rdd = sc.parallelize(data)\n",
    "\n",
    "    // Show the original RDD\n",
    "    println(\"Original RDD:\")\n",
    "    rdd.collect().foreach(println)\n",
    "    //rdd = rdd.filter{ case (name, age) => age > 30 }\n",
    "    // // Filter rows where the age is greater than 30\n",
    "    val filteredRdd = rdd.filter{ case (name, age) => age > 30 }\n",
    "    println(s\"Transformed RDD ID: ${filteredRdd.id}\")\n",
    "\n",
    "    // Show the transformed RDD\n",
    "    println(\"Filtered RDD:\")\n",
    "    filteredRdd.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec3c288-82fc-49ea-9e92-7486a917260a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark Lazy Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3ad62f-3dfa-4c7b-901a-08bd8718d564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Smith</td><td>44</td></tr><tr><td>Adam</td><td>65</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Smith",
         44
        ],
        [
         "Adam",
         65
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([\n",
    "    (\"John\", 28),\n",
    "    (\"Smith\", 44),\n",
    "    (\"Adam\", 65),\n",
    "    (\"Henry\", 23)\n",
    "])\n",
    "\n",
    "# Apply a map transformation to create a new RDD with a tuple including the name and a boolean flag\n",
    "# if the person is older than 30\n",
    "mapped_rdd = rdd.map(lambda x: (x[0], x[1], x[1] > 30))\n",
    "\n",
    "# Filter the RDD to include only people older than 30\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x[2])\n",
    "\n",
    "# Convert the filtered RDD back to a DataFrame\n",
    "df = spark.createDataFrame(filtered_rdd, [\"Name\", \"Age\", \"OlderThan30\"])\n",
    "\n",
    "# Select only the name and age columns\n",
    "final_df = df.select(\"Name\", \"Age\")\n",
    "\n",
    "# # Collect the results which triggers the execution of all transformations\n",
    "results = final_df.collect()\n",
    "display(results)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "22-spark-immutablility-example",
   "widgets": {}
  },
  "jupytext": {
   "formats": "ipynb,md"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
